{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# <center> **TP2 INF4248: RNN+LSTM+Text**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<center> Members\n",
        "\n",
        " ## <center> **Hapi Kamgang Franck  ( 18T2418 )**\n",
        " ## <center> **Ngoran Aristide Fondzela ( 20V2896 )**\n",
        " ## <center> **NUNMUA SONTSA Belvanie Kartel ( 19M2319 )**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NyG9X7gORbI-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlZLZIxlRZ9h",
        "outputId": "86d48c59-5028-4ccc-b394-074fa8697c1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Connextion de colab au google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importation des bibliotheques\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "cme7SC-yR4NV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chargement du dataset\n",
        "filename = \"/content/drive/MyDrive/wonderland.txt\"\n",
        "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
        "raw_text = raw_text.lower()"
      ],
      "metadata": {
        "id": "8_2YKAvycsmW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ],
      "metadata": {
        "id": "4h7mD6DregPR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars) # nombre total de caracteres contenu dans le document\n",
        "print(\"Total Vocab: \", n_vocab) # Nombre total de caractere distinct contenu dans le document apres convertion des caracteres en minuscules"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugEkag_Bf6sD",
        "outputId": "be2ec159-50d0-459b-9363-b909376747f7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Characters:  144584\n",
            "Total Vocab:  49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "  seq_in = raw_text[i:i + seq_length]\n",
        "  seq_out = raw_text[i + seq_length]\n",
        "  dataX.append([char_to_int[char] for char in seq_in])\n",
        "  dataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ULhzr-RgOCs",
        "outputId": "d61c87f0-42e0-4653-bc1c-7b9fb8e5437d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Patterns:  144484\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les LSTM, attendent en entrée des donnees sous un format à 3 dimensions. Cela permet au modèle de traiter des séquences d'informations et de capturer les relations entre les caractères au sein d'une séquence.\n",
        "En remodelant dataX dans ce format, on prépare nos donnees à être introduites dans un modèle LSTM pour l'apprentissage."
      ],
      "metadata": {
        "id": "05MQ2EvCIGAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = to_categorical(dataY)"
      ],
      "metadata": {
        "id": "YGBIhOQViY37"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Réseau de neuronal récurrent LSTM**"
      ],
      "metadata": {
        "id": "TJ1XBaFjYQAn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Definir le model**"
      ],
      "metadata": {
        "id": "CMZfXbCKkic7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the LSTM(Long Short-Term Memory) model\n",
        "\n",
        "# 256: Correspond au nombre neurones cachés dans la couche LSTM.\n",
        "# Ces neurones représentent la mémoire interne du réseau et permettent de capturer les dépendances à long terme entre les caractères dans les séquences textuelles.\n",
        "# input_shape=(X.shape[1], X.shape[2]): Définit la forme des données d'entrée attendues par la couche LSTM.\n",
        "# X.shape[1]: Correspond à la longueur des séquences d'entrée (nombre de caractères par séquence dans X).\n",
        "# X.shape[2]: Correspond au nombre de caractéristiques par caractère (dans ce cas, on suppose que chaque caractère est représenté par une seule valeur, donc 1).\n",
        "\n",
        "# 0.2: Indique le taux de désactivation (dropout rate) de 20%.\n",
        "# Le dropout permet de désactiver aléatoirement 20% des neurones à chaque itération d'entraînement, ce qui aide à prévenir le surapprentissage (overfitting) du modèle.\n",
        "\n",
        "# y.shape[1]: Correspond au nombre de neurones dans la couche dense, qui doit être égal au nombre de classes de sortie (nombre de caractères possibles dans le vocabulaire).\n",
        "# activation='softmax': Définit la fonction d'activation de la couche dense qui normalise les sorties de la couche en des probabilités comprises entre 0 et 1.\n",
        "# Chaque sortie représente la probabilité que le caractère suivant appartienne à une classe particulière.\n",
        "\n",
        "# La compilation configure le modèle pour l'entraînement en spécifiant la fonction de perte, l'optimiseur et d'autres paramètres.\n",
        "# loss='categorical_crossentropy': Définit la fonction de perte à minimiser pendant l'entraînement.\n",
        "# optimizer='adam': Définit l'optimiseur utilisé pour mettre à jour les poids du réseau pendant l'entraînement.\n",
        "# Adam est un optimiseur populaire qui ajuste efficacement les poids du réseau.\n",
        "\n",
        "model = Sequential() # Crée un modèle séquentiel vide\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]))) # Ajoute une couche LSTM au modèle\n",
        "model.add(Dropout(0.2)) # Ajoute une couche de dropout au modèle.\n",
        "model.add(Dense(y.shape[1], activation='softmax')) # Ajoute une couche dense au modèle.\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam') # Compile le modèle.\n",
        "\n",
        "# Définit le chemin d'accès pour sauvegarder les poids du modèle.\n",
        "# {epoch:02d}: Sera remplacé par le numéro de l'époque d'entraînement en cours (formatté sur 2 chiffres).\n",
        "# {loss:.4f}: Sera remplacé par la valeur de la perte à la fin de l'époque (formatté sur 4 chiffres après la virgule).\n",
        "# \".keras\": Extension de fichier des poids du modèle.\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.keras\"\n",
        "\n",
        "# Crée un objet ModelCheckpoint.\n",
        "# filepath: Spécifie le chemin d'accès défini précédemment.\n",
        "# monitor='loss': Indique que l'on surveille la valeur de la perte pendant l'entraînement.\n",
        "# verbose=1: Affiche un message d'information à chaque fois qu'un nouveau meilleur poids est sauvegardé.\n",
        "# save_best_only=True: Sauvegarde uniquement les poids du modèle qui correspondent à la plus faible valeur de la perte observée jusqu'à présent.\n",
        "# mode='min': Indique que l'on cherche à minimiser la valeur de la perte (on sauvegarde le modèle avec la plus faible perte).\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint] # Crée une liste contenant l'objet ModelCheckpoint.\n",
        "\n",
        "\n",
        "# Entrainement du modele\n",
        "# X : Ce sont les données d'entraînement, collection de séquences d'entiers (représentant des caractères) à partir desquelles le modèle apprendra..\n",
        "# y : Ce sont les données cibles. Elles contiennent le caractère suivant dans la séquence pour chaque exemple d'entraînement présent dans X.\n",
        "# epochs=20 : Cela spécifie le nombre d'époques d'entraînement. Une époque correspond à un passage complet sur l'ensemble des données d'entraînement.\n",
        "# batch_size=128 : Ceci définit la taille du lot (batch size). La taille du lot détermine le nombre d'exemples d'entraînement traités par le modèle lors d'une seule mise à jour pendant l'entraînement.\n",
        "# callbacks=callbacks_list : Cela spécifie une liste de fonctions de rappel (callback functions) qui sont invoquées pendant le processus d'entraînement. Ici, callbacks_list contient l'objet ModelCheckpoint défini précédemment.\n",
        "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lo1Ps8OfknfB",
        "outputId": "1601efe3-9804-4f9d-a2f8-622700b1212d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1128/1129 [============================>.] - ETA: 0s - loss: 2.9990\n",
            "Epoch 1: loss improved from inf to 2.99868, saving model to weights-improvement-01-2.9987.keras\n",
            "1129/1129 [==============================] - 17s 13ms/step - loss: 2.9987\n",
            "Epoch 2/20\n",
            "1126/1129 [============================>.] - ETA: 0s - loss: 2.7995\n",
            "Epoch 2: loss improved from 2.99868 to 2.79957, saving model to weights-improvement-02-2.7996.keras\n",
            "1129/1129 [==============================] - 15s 13ms/step - loss: 2.7996\n",
            "Epoch 3/20\n",
            "1129/1129 [==============================] - ETA: 0s - loss: 2.6995\n",
            "Epoch 3: loss improved from 2.79957 to 2.69947, saving model to weights-improvement-03-2.6995.keras\n",
            "1129/1129 [==============================] - 15s 13ms/step - loss: 2.6995\n",
            "Epoch 4/20\n",
            "1127/1129 [============================>.] - ETA: 0s - loss: 2.6224\n",
            "Epoch 4: loss improved from 2.69947 to 2.62218, saving model to weights-improvement-04-2.6222.keras\n",
            "1129/1129 [==============================] - 15s 13ms/step - loss: 2.6222\n",
            "Epoch 5/20\n",
            "1126/1129 [============================>.] - ETA: 0s - loss: 2.5539\n",
            "Epoch 5: loss improved from 2.62218 to 2.55382, saving model to weights-improvement-05-2.5538.keras\n",
            "1129/1129 [==============================] - 14s 13ms/step - loss: 2.5538\n",
            "Epoch 6/20\n",
            "1126/1129 [============================>.] - ETA: 0s - loss: 2.4926\n",
            "Epoch 6: loss improved from 2.55382 to 2.49248, saving model to weights-improvement-06-2.4925.keras\n",
            "1129/1129 [==============================] - 14s 13ms/step - loss: 2.4925\n",
            "Epoch 7/20\n",
            "1127/1129 [============================>.] - ETA: 0s - loss: 2.4355\n",
            "Epoch 7: loss improved from 2.49248 to 2.43575, saving model to weights-improvement-07-2.4358.keras\n",
            "1129/1129 [==============================] - 14s 13ms/step - loss: 2.4358\n",
            "Epoch 8/20\n",
            "1129/1129 [==============================] - ETA: 0s - loss: 2.3973\n",
            "Epoch 8: loss improved from 2.43575 to 2.39735, saving model to weights-improvement-08-2.3973.keras\n",
            "1129/1129 [==============================] - 15s 13ms/step - loss: 2.3973\n",
            "Epoch 9/20\n",
            "1128/1129 [============================>.] - ETA: 0s - loss: 2.3503\n",
            "Epoch 9: loss improved from 2.39735 to 2.35011, saving model to weights-improvement-09-2.3501.keras\n",
            "1129/1129 [==============================] - 15s 13ms/step - loss: 2.3501\n",
            "Epoch 10/20\n",
            "1127/1129 [============================>.] - ETA: 0s - loss: 2.3181\n",
            "Epoch 10: loss improved from 2.35011 to 2.31792, saving model to weights-improvement-10-2.3179.keras\n",
            "1129/1129 [==============================] - 14s 13ms/step - loss: 2.3179\n",
            "Epoch 11/20\n",
            "1128/1129 [============================>.] - ETA: 0s - loss: 2.2781\n",
            "Epoch 11: loss improved from 2.31792 to 2.27806, saving model to weights-improvement-11-2.2781.keras\n",
            "1129/1129 [==============================] - 14s 13ms/step - loss: 2.2781\n",
            "Epoch 12/20\n",
            "1128/1129 [============================>.] - ETA: 0s - loss: 2.2463\n",
            "Epoch 12: loss improved from 2.27806 to 2.24622, saving model to weights-improvement-12-2.2462.keras\n",
            "1129/1129 [==============================] - 14s 13ms/step - loss: 2.2462\n",
            "Epoch 13/20\n",
            "1126/1129 [============================>.] - ETA: 0s - loss: 2.2100\n",
            "Epoch 13: loss improved from 2.24622 to 2.21034, saving model to weights-improvement-13-2.2103.keras\n",
            "1129/1129 [==============================] - 14s 13ms/step - loss: 2.2103\n",
            "Epoch 14/20\n",
            "1129/1129 [==============================] - ETA: 0s - loss: 2.1791\n",
            "Epoch 14: loss improved from 2.21034 to 2.17905, saving model to weights-improvement-14-2.1791.keras\n",
            "1129/1129 [==============================] - 14s 13ms/step - loss: 2.1791\n",
            "Epoch 15/20\n",
            "1128/1129 [============================>.] - ETA: 0s - loss: 2.1369\n",
            "Epoch 15: loss improved from 2.17905 to 2.13688, saving model to weights-improvement-15-2.1369.keras\n",
            "1129/1129 [==============================] - 14s 13ms/step - loss: 2.1369\n",
            "Epoch 16/20\n",
            "1127/1129 [============================>.] - ETA: 0s - loss: 2.1053\n",
            "Epoch 16: loss improved from 2.13688 to 2.10512, saving model to weights-improvement-16-2.1051.keras\n",
            "1129/1129 [==============================] - 15s 13ms/step - loss: 2.1051\n",
            "Epoch 17/20\n",
            "1127/1129 [============================>.] - ETA: 0s - loss: 2.0715\n",
            "Epoch 17: loss improved from 2.10512 to 2.07172, saving model to weights-improvement-17-2.0717.keras\n",
            "1129/1129 [==============================] - 15s 13ms/step - loss: 2.0717\n",
            "Epoch 18/20\n",
            "1126/1129 [============================>.] - ETA: 0s - loss: 2.0405\n",
            "Epoch 18: loss improved from 2.07172 to 2.04054, saving model to weights-improvement-18-2.0405.keras\n",
            "1129/1129 [==============================] - 15s 13ms/step - loss: 2.0405\n",
            "Epoch 19/20\n",
            "1128/1129 [============================>.] - ETA: 0s - loss: 2.0128\n",
            "Epoch 19: loss improved from 2.04054 to 2.01303, saving model to weights-improvement-19-2.0130.keras\n",
            "1129/1129 [==============================] - 15s 13ms/step - loss: 2.0130\n",
            "Epoch 20/20\n",
            "1126/1129 [============================>.] - ETA: 0s - loss: 1.9862\n",
            "Epoch 20: loss improved from 2.01303 to 1.98612, saving model to weights-improvement-20-1.9861.keras\n",
            "1129/1129 [==============================] - 15s 13ms/step - loss: 1.9861\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79c665775ae0>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Génération de texte à l'aide du réseau LSTM formé**"
      ],
      "metadata": {
        "id": "_cCXhlOnWzKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the network weights\n",
        "filename = \"/content/weights-improvement-20-1.9861.keras\"\n",
        "model.load_weights(filename)\n",
        "\n",
        "# pick a random seed\n",
        "start = np.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "# generate characters\n",
        "for i in range(1000):\n",
        "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = np.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxhOimPW0vsk",
        "outputId": "300faea8-dd7c-4722-a507-74d724291b18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Characters:  144584\n",
            "Total Vocab:  49\n",
            "Total Patterns:  144484\n",
            "Seed:\n",
            "\" rning,” shouted the queen, stamping on the\n",
            "ground as she spoke; “either you or your head must be off \"\n",
            "333l333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Réseau de neuronal récurrent LSTM plus grand**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6U7yrK0nX4Mg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Larger LSTM Network to Generate Text for Alice in Wonderland\n",
        "\n",
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# define the checkpoint\n",
        "filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.keras\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "# fit the model\n",
        "model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)"
      ],
      "metadata": {
        "id": "Olr1wnty1CH4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "696010c5-3790-4c4b-d6f9-53273302de64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Characters:  144584\n",
            "Total Vocab:  49\n",
            "Total Patterns:  144484\n",
            "Epoch 1/50\n",
            "2258/2258 [==============================] - ETA: 0s - loss: 2.8262\n",
            "Epoch 1: loss improved from inf to 2.82625, saving model to weights-improvement-01-2.8262-bigger.keras\n",
            "2258/2258 [==============================] - 46s 19ms/step - loss: 2.8262\n",
            "Epoch 2/50\n",
            "2256/2258 [============================>.] - ETA: 0s - loss: 2.4331\n",
            "Epoch 2: loss improved from 2.82625 to 2.43285, saving model to weights-improvement-02-2.4329-bigger.keras\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 2.4329\n",
            "Epoch 3/50\n",
            "2256/2258 [============================>.] - ETA: 0s - loss: 2.2316\n",
            "Epoch 3: loss improved from 2.43285 to 2.23151, saving model to weights-improvement-03-2.2315-bigger.keras\n",
            "2258/2258 [==============================] - 42s 19ms/step - loss: 2.2315\n",
            "Epoch 4/50\n",
            "2257/2258 [============================>.] - ETA: 0s - loss: 2.0980\n",
            "Epoch 4: loss improved from 2.23151 to 2.09802, saving model to weights-improvement-04-2.0980-bigger.keras\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 2.0980\n",
            "Epoch 5/50\n",
            "2256/2258 [============================>.] - ETA: 0s - loss: 2.0039\n",
            "Epoch 5: loss improved from 2.09802 to 2.00390, saving model to weights-improvement-05-2.0039-bigger.keras\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 2.0039\n",
            "Epoch 6/50\n",
            "2257/2258 [============================>.] - ETA: 0s - loss: 1.9261\n",
            "Epoch 6: loss improved from 2.00390 to 1.92614, saving model to weights-improvement-06-1.9261-bigger.keras\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.9261\n",
            "Epoch 7/50\n",
            "2257/2258 [============================>.] - ETA: 0s - loss: 1.8652\n",
            "Epoch 7: loss improved from 1.92614 to 1.86524, saving model to weights-improvement-07-1.8652-bigger.keras\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.8652\n",
            "Epoch 8/50\n",
            "2257/2258 [============================>.] - ETA: 0s - loss: 1.8155\n",
            "Epoch 8: loss improved from 1.86524 to 1.81547, saving model to weights-improvement-08-1.8155-bigger.keras\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.8155\n",
            "Epoch 9/50\n",
            "2257/2258 [============================>.] - ETA: 0s - loss: 1.8041\n",
            "Epoch 9: loss improved from 1.81547 to 1.80413, saving model to weights-improvement-09-1.8041-bigger.keras\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.8041\n",
            "Epoch 10/50\n",
            "2257/2258 [============================>.] - ETA: 0s - loss: 1.7275\n",
            "Epoch 10: loss improved from 1.80413 to 1.72748, saving model to weights-improvement-10-1.7275-bigger.keras\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.7275\n",
            "Epoch 11/50\n",
            "2257/2258 [============================>.] - ETA: 0s - loss: 1.7750\n",
            "Epoch 11: loss did not improve from 1.72748\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.7750\n",
            "Epoch 12/50\n",
            "2257/2258 [============================>.] - ETA: 0s - loss: 1.7671\n",
            "Epoch 12: loss did not improve from 1.72748\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.7672\n",
            "Epoch 13/50\n",
            "2256/2258 [============================>.] - ETA: 0s - loss: 1.6400\n",
            "Epoch 13: loss improved from 1.72748 to 1.64008, saving model to weights-improvement-13-1.6401-bigger.keras\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.6401\n",
            "Epoch 14/50\n",
            "2257/2258 [============================>.] - ETA: 0s - loss: 1.6158\n",
            "Epoch 14: loss improved from 1.64008 to 1.61591, saving model to weights-improvement-14-1.6159-bigger.keras\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.6159\n",
            "Epoch 15/50\n",
            "2257/2258 [============================>.] - ETA: 0s - loss: 1.6663\n",
            "Epoch 15: loss did not improve from 1.61591\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.6663\n",
            "Epoch 16/50\n",
            "2257/2258 [============================>.] - ETA: 0s - loss: 1.5651\n",
            "Epoch 16: loss improved from 1.61591 to 1.56506, saving model to weights-improvement-16-1.5651-bigger.keras\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.5651\n",
            "Epoch 17/50\n",
            "2257/2258 [============================>.] - ETA: 0s - loss: 1.5462\n",
            "Epoch 17: loss improved from 1.56506 to 1.54617, saving model to weights-improvement-17-1.5462-bigger.keras\n",
            "2258/2258 [==============================] - 42s 19ms/step - loss: 1.5462\n",
            "Epoch 18/50\n",
            "2256/2258 [============================>.] - ETA: 0s - loss: 1.5225\n",
            "Epoch 18: loss improved from 1.54617 to 1.52280, saving model to weights-improvement-18-1.5228-bigger.keras\n",
            "2258/2258 [==============================] - 42s 18ms/step - loss: 1.5228\n",
            "Epoch 19/50\n",
            "2257/2258 [============================>.] - ETA: 0s - loss: 1.5078\n",
            "Epoch 19: loss improved from 1.52280 to 1.50787, saving model to weights-improvement-19-1.5079-bigger.keras\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.5079\n",
            "Epoch 20/50\n",
            "2257/2258 [============================>.] - ETA: 0s - loss: 1.4940\n",
            "Epoch 20: loss improved from 1.50787 to 1.49395, saving model to weights-improvement-20-1.4940-bigger.keras\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.4940\n",
            "Epoch 21/50\n",
            "2256/2258 [============================>.] - ETA: 0s - loss: 1.4632\n",
            "Epoch 21: loss improved from 1.49395 to 1.46331, saving model to weights-improvement-21-1.4633-bigger.keras\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.4633\n",
            "Epoch 22/50\n",
            "2257/2258 [============================>.] - ETA: 0s - loss: 1.4818\n",
            "Epoch 22: loss did not improve from 1.46331\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.4818\n",
            "Epoch 23/50\n",
            "2257/2258 [============================>.] - ETA: 0s - loss: 1.4550\n",
            "Epoch 23: loss improved from 1.46331 to 1.45501, saving model to weights-improvement-23-1.4550-bigger.keras\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.4550\n",
            "Epoch 24/50\n",
            "2257/2258 [============================>.] - ETA: 0s - loss: 1.4706\n",
            "Epoch 24: loss did not improve from 1.45501\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.4706\n",
            "Epoch 25/50\n",
            "2257/2258 [============================>.] - ETA: 0s - loss: 1.4381\n",
            "Epoch 25: loss improved from 1.45501 to 1.43807, saving model to weights-improvement-25-1.4381-bigger.keras\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.4381\n",
            "Epoch 26/50\n",
            "2256/2258 [============================>.] - ETA: 0s - loss: 1.3998\n",
            "Epoch 26: loss improved from 1.43807 to 1.39966, saving model to weights-improvement-26-1.3997-bigger.keras\n",
            "2258/2258 [==============================] - 42s 18ms/step - loss: 1.3997\n",
            "Epoch 27/50\n",
            "2257/2258 [============================>.] - ETA: 0s - loss: 1.3842\n",
            "Epoch 27: loss improved from 1.39966 to 1.38429, saving model to weights-improvement-27-1.3843-bigger.keras\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.3843\n",
            "Epoch 28/50\n",
            "2257/2258 [============================>.] - ETA: 0s - loss: 1.3756\n",
            "Epoch 28: loss improved from 1.38429 to 1.37558, saving model to weights-improvement-28-1.3756-bigger.keras\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.3756\n",
            "Epoch 29/50\n",
            "2257/2258 [============================>.] - ETA: 0s - loss: 1.3675\n",
            "Epoch 29: loss improved from 1.37558 to 1.36746, saving model to weights-improvement-29-1.3675-bigger.keras\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.3675\n",
            "Epoch 30/50\n",
            "2257/2258 [============================>.] - ETA: 0s - loss: 1.3548\n",
            "Epoch 30: loss improved from 1.36746 to 1.35502, saving model to weights-improvement-30-1.3550-bigger.keras\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.3550\n",
            "Epoch 31/50\n",
            "2256/2258 [============================>.] - ETA: 0s - loss: 1.3491\n",
            "Epoch 31: loss improved from 1.35502 to 1.34914, saving model to weights-improvement-31-1.3491-bigger.keras\n",
            "2258/2258 [==============================] - 42s 19ms/step - loss: 1.3491\n",
            "Epoch 32/50\n",
            "2257/2258 [============================>.] - ETA: 0s - loss: 1.3353\n",
            "Epoch 32: loss improved from 1.34914 to 1.33531, saving model to weights-improvement-32-1.3353-bigger.keras\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.3353\n",
            "Epoch 33/50\n",
            "2257/2258 [============================>.] - ETA: 0s - loss: 1.3396\n",
            "Epoch 33: loss did not improve from 1.33531\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.3396\n",
            "Epoch 34/50\n",
            "2257/2258 [============================>.] - ETA: 0s - loss: 1.3255\n",
            "Epoch 34: loss improved from 1.33531 to 1.32564, saving model to weights-improvement-34-1.3256-bigger.keras\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.3256\n",
            "Epoch 35/50\n",
            "2257/2258 [============================>.] - ETA: 0s - loss: 1.3449\n",
            "Epoch 35: loss did not improve from 1.32564\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.3450\n",
            "Epoch 36/50\n",
            "2256/2258 [============================>.] - ETA: 0s - loss: 1.3721\n",
            "Epoch 36: loss did not improve from 1.32564\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.3720\n",
            "Epoch 37/50\n",
            "2258/2258 [==============================] - ETA: 0s - loss: 1.3298\n",
            "Epoch 37: loss did not improve from 1.32564\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.3298\n",
            "Epoch 38/50\n",
            "2257/2258 [============================>.] - ETA: 0s - loss: 1.3125\n",
            "Epoch 38: loss improved from 1.32564 to 1.31255, saving model to weights-improvement-38-1.3126-bigger.keras\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.3126\n",
            "Epoch 39/50\n",
            "2257/2258 [============================>.] - ETA: 0s - loss: 1.3062\n",
            "Epoch 39: loss improved from 1.31255 to 1.30627, saving model to weights-improvement-39-1.3063-bigger.keras\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.3063\n",
            "Epoch 40/50\n",
            "2256/2258 [============================>.] - ETA: 0s - loss: 1.3001\n",
            "Epoch 40: loss improved from 1.30627 to 1.30037, saving model to weights-improvement-40-1.3004-bigger.keras\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.3004\n",
            "Epoch 41/50\n",
            "2257/2258 [============================>.] - ETA: 0s - loss: 1.3017\n",
            "Epoch 41: loss did not improve from 1.30037\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.3017\n",
            "Epoch 42/50\n",
            "2257/2258 [============================>.] - ETA: 0s - loss: 1.2939\n",
            "Epoch 42: loss improved from 1.30037 to 1.29385, saving model to weights-improvement-42-1.2938-bigger.keras\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.2938\n",
            "Epoch 43/50\n",
            "2257/2258 [============================>.] - ETA: 0s - loss: 1.2852\n",
            "Epoch 43: loss improved from 1.29385 to 1.28516, saving model to weights-improvement-43-1.2852-bigger.keras\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.2852\n",
            "Epoch 44/50\n",
            "2257/2258 [============================>.] - ETA: 0s - loss: 1.2809\n",
            "Epoch 44: loss improved from 1.28516 to 1.28105, saving model to weights-improvement-44-1.2811-bigger.keras\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.2811\n",
            "Epoch 45/50\n",
            "2257/2258 [============================>.] - ETA: 0s - loss: 1.2733\n",
            "Epoch 45: loss improved from 1.28105 to 1.27329, saving model to weights-improvement-45-1.2733-bigger.keras\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.2733\n",
            "Epoch 46/50\n",
            "2256/2258 [============================>.] - ETA: 0s - loss: 1.2701\n",
            "Epoch 46: loss improved from 1.27329 to 1.27018, saving model to weights-improvement-46-1.2702-bigger.keras\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.2702\n",
            "Epoch 47/50\n",
            "2257/2258 [============================>.] - ETA: 0s - loss: 1.2670\n",
            "Epoch 47: loss improved from 1.27018 to 1.26702, saving model to weights-improvement-47-1.2670-bigger.keras\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.2670\n",
            "Epoch 48/50\n",
            "2257/2258 [============================>.] - ETA: 0s - loss: 1.2622\n",
            "Epoch 48: loss improved from 1.26702 to 1.26211, saving model to weights-improvement-48-1.2621-bigger.keras\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.2621\n",
            "Epoch 49/50\n",
            "2257/2258 [============================>.] - ETA: 0s - loss: 1.2624\n",
            "Epoch 49: loss did not improve from 1.26211\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.2624\n",
            "Epoch 50/50\n",
            "2257/2258 [============================>.] - ETA: 0s - loss: 1.2640\n",
            "Epoch 50: loss did not improve from 1.26211\n",
            "2258/2258 [==============================] - 41s 18ms/step - loss: 1.2640\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79c6672b9870>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Génération de texte à l'aide du réseau LSTM plus grand formé**"
      ],
      "metadata": {
        "id": "0pfQl1UcZxvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Larger LSTM network and generate text\n",
        "\n",
        "# load the network weights\n",
        "filename = \"/content/weights-improvement-48-1.2621-bigger.keras\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "# pick a random seed\n",
        "start = np.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "# generate characters\n",
        "for i in range(1000):\n",
        "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = np.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oeik-N6SY7H",
        "outputId": "1d331268-1097-46a2-cedc-9f01f820d56d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Characters:  144584\n",
            "Total Vocab:  49\n",
            "Total Patterns:  144484\n",
            "Seed:\n",
            "\" riting very busily on slates. “what are\n",
            "they doing?” alice whispered to the gryphon. “they can’t hav \"\n",
            "e to say it out of the tame with its here, what is is?”\n",
            "\n",
            "“i don’t know what the pabbit hirst the beginning about it,” said the cat. \n",
            "“aod that it advantar that it wastend your pardon!” said the cat. \n",
            "“and that it advantar the earth, the dould not a courle?” she said to herself, “it would be wery such a baby, i shall see the way out of the tame was oot a bit, if i can’t tell you mike them!”\n",
            "\n",
            "“i don’t know what the pueen of the great was into a pames tay in the same with the torm!”\n",
            "\n",
            "“i wonder what i say that is as all?” said the mock turtle. \n",
            "“nf course they dres the way it is a poot as the same thing to be a louse the season in the sea. “i don’t think the words don’t be none to sea to as the same with the toiak? but i co sert like them that _shat_ iis seall! the doumta way in my time?”\n",
            "\n",
            "“i don’t know what the pabbit hirst the beginning about it,” said the cat. \n",
            "“aod that it advantar that it wastend your pardon!” said the cat. \n",
            "“and that it advantar the earth, the dould not a courle?” sh\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XeQ-yqcmizrF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}